# -*- coding: utf-8 -*-
"""code 02_production_usingImageSearch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Pv6Uu9jkJRG9mmkD_H_wj78uLG4uWmF
"""

!pip install -Uqq fastbook
#!pip install -q jmd_imagescraper

import fastbook
fastbook.setup_book()

from fastbook import *
from fastai.vision.widgets import *
from pathlib import Path

"""## Gathering Data Using Duckduckgo Search


"""

urls_grizzly = search_images_ddg("grizzly bear", max_images=50)

urls_grizzly

type(urls_grizzly)

urls_grizzly[0]

urls_black = search_images_ddg("black bear", max_images=50)
urls_teddy = search_images_ddg("teddy bear", max_images=50)

urls_grizzly

#path = Path('/content/gdrive/MyDrive/Colab Notebooks/data/bears/')

path = Path('./sample_data/bears/')
path_grizzly = path/'grizzly/'
path_black = path/'black/'
path_teddy = path/'teddy/'

#path_grizzly = Path('./sample_data/bears/grizzly/')
#path_black = Path('./sample_data/bears/black/')
#path_teddy = Path('./sample_data/bears/teddy/')

!pwd

path_grizzly

path_grizzly.mkdir(exist_ok=True, parents=True)
path_black.mkdir(exist_ok=True, parents=True)
path_teddy.mkdir(exist_ok=True, parents=True)

download_url(urls_grizzly[1], path_grizzly)

for o in urls_grizzly:
  try:
      urlopen(o)
      download_url(o, path_grizzly)
  except:
    pass

for o in urls_black:
  try:
      urlopen(o)
      download_url(o, path_black)
  except:
    pass

for o in urls_teddy:
  try:
      urlopen(o)
      download_url(o, path_teddy)
  except:
    pass

fns_grizzly = get_image_files(path_grizzly)

fns_grizzly

fns_black = get_image_files(path_black)
fns_teddy = get_image_files(path_teddy)
len(fns_teddy), len(fns_black), len(fns_grizzly),

type(fns_black)

fns_teddy

failed = verify_images(fns_teddy)
failed

# To remove all the failed images, you can use unlink on each of them.
# "verify_images" returns an object of type L, which includes the map method.
failed.map(Path.unlink);

failed = verify_images(fns_black)
failed

failed.map(Path.unlink)

failed = verify_images(fns_grizzly)
failed

failed.map(Path.unlink)

doc(verify_images)
# This tells us what argument the function accepts (fns), then shows us the source code and the file it comes from.
# Looking at that source code, we can see it applies the function verify_image in parallel and only keeps the image files for which the result of that function is False, which is consistent with the doc string: it finds the images in fns that can't be opened.

# Here are some other features that are very useful in Jupyter notebooks:
# you can press Tab to get autocompletion suggestions.
# When inside the parentheses of a function, pressing Shift and Tab simultaneously will display a window with the signature of the function and a short description.
# Pressing these keys twice will expand the documentation, and pressing them three times will open a full window with the same information at the bottom of your screen.
# In a cell, typing ?func_name and executing will open a window with the signature of the function and a short description.
# In a cell, typing ??func_name and executing will open a window with the signature of the function, a short description, and the source code.
# If you are using the fastai library, we added a doc function for you: executing doc(func_name) in a cell will open a window with the signature of the function, a short description and links to the source code on GitHub and the full documentation of the function in the library docs.
# Unrelated to the documentation but still very useful: to get help at any point if you get an error, type %debug in the next cell and execute to open the Python debugger, which will let you inspect the content of every variable.

"""## From Data to DataLoaders"""

path

dlsMethod = ImageDataLoaders.from_folder(path, valid_pct=0.2, seed=42, item_tfms=Resize(128))

dlsMethod

type(dlsMethod)

dlsMethod.valid.show_batch(max_n=18, nrows=3)

# data block API: Using this, you can fully customize every stage of the creation of DataLoaders
bears = DataBlock(
    blocks=(ImageBlock, CategoryBlock),
    get_items=get_image_files,
    splitter=RandomSplitter(valid_pct=0.2, seed=42),
    get_y=parent_label,
    item_tfms=Resize(128))

bears

type(bears)

#dlsAPI = bears.dataloaders(bearImagepath)
dlsAPI = bears.dataloaders(path)

dlsAPI

dlsAPI.train.show_batch(max_n=18, nrows=3)

dlsAPI.valid.show_batch(max_n=18, nrows=3)

type(dlsAPI)

dlsAPI

dlsMethod

#dls = bears.dataloaders(path)
#dls.train.show_batch(max_n=18, nrows=3)
#dls.valid.show_batch(max_n=18, nrows=3)

bears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish))
dls = bears.dataloaders(path)
dls.valid.show_batch(max_n=18, nrows=3)

bears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))
dls = bears.dataloaders(path)
dls.valid.show_batch(max_n=18, nrows=3)

bears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))
dls = bears.dataloaders(path)
dls.train.show_batch(max_n=24, nrows=4, unique=True)

"""### Data Augmentation"""

bears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))
# rotation, flipping, perspective warping, brightness and contrast changes
dls = bears.dataloaders(path)
dls.train.show_batch(max_n=24, nrows=4, unique=True)

"""## Training Your Model, and Using It to Clean Your Data"""

# We don't have a lot of data for our problem
# So to train our model, we'll use `RandomResizedCrop` with an image size of 224 px,
# which is fairly standard for image classification, and default `aug_transforms`:
bears = bears.new(
    item_tfms=RandomResizedCrop(224, min_scale=0.5),
    batch_tfms=aug_transforms())
dls = bears.dataloaders(path)

len(dls.valid_ds),len(dls.train_ds)

learn = cnn_learner(dls, resnet18, metrics=error_rate)
learn.fine_tune(12)

learn.model

interp = ClassificationInterpretation.from_learner(learn)
interp.plot_confusion_matrix()

losses,idxs = interp.top_losses()

losses,idxs

# The loss is a number that is higher if the model is incorrect (especially if it's also confident of its incorrect answer),
# or if it's correct, but not confident of its correct answer.
# In a couple of chapters we'll learn in depth how loss is calculated and used in the training process.
# For now, plot_top_losses shows us the images with the highest loss in our dataset.
# As the title of the output says, each image is labeled with four things: prediction, actual (target label), loss, and probability.
# The probability here is the confidence level, from zero to one, that the model has assigned to its prediction:
interp.plot_top_losses(3, nrows=3)

interp.plot_top_losses(10, nrows=10)

from google.colab import drive
drive.mount('/content/gdrive')

uploader = SimpleNamespace(data = ['/content/gdrive/MyDrive/Colab Notebooks/data/images/cat.png'])
#img = PILImage.create(uploader.data[0])
img = Image.open(uploader.data[0])
img.to_thumb(192)

classification,_,probs = learn.predict(img)
print(f"Which class? {classification}")
print(f"Probabilities for black: {probs[0].item():.3f}, grizzly: {probs[1].item():.3f}, and teddy: {probs[2].item():.3f}")